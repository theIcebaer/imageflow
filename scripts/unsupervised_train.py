# -*- coding: utf-8 -*-
"""setup_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fabj3TRxAuRU00-ePGmGtgdYAaCvSXmx

Dies hier ist ein kleines experiment, dass überprüfen soll, ob ein cinn auch mit einer Mischung aus GAN setup und
reconstruction loss trainiert werden kann.
Dazu wird zunächst ein sample für ein Geschwindigkeitsfeld generiert angewendet und an ein reconstruction loss
berechnet. Die erste wichtige frage ist, ob anschließend noch ein nll rückwärts berechnet werden muss.
"""

import os
import datetime
import torch
# import tensorflow as tf
# import matplotlib.pyplot as plt
#
# from voxelmorph.torch.layers import VecInt
# from voxelmorph.torch.layers import SpatialTransformer

from src.nets import Reg_mnist_cINN
from src.dataset import MnistDataset

# -- settings ---------------------------------------------------------------------------------------------------------

device = torch.device('cpu')
batch_size = 64
val_batch_size = 1024
n_epochs = 60

augm_sigma = 0.08

image_shape = (28, 28)
field_shape = (2, 28, 28)
ndim_total = 28*28*2
plot = True
base_dir = "../"
run_dir = os.path.join(base_dir, "runs", datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))
data_dir = os.path.join(base_dir, "data")

# ---------------------------------------------------------------------------------------------------------------------

print("Preparing the data loaders...")
data_set = MnistDataset(os.path.join(data_dir, "mnist_rnd_distortions_1.hdf5"))
train_set, val_set, test_set = torch.utils.data.random_split(data_set, [47712, 4096, 8192],
                                                             generator=torch.Generator().manual_seed(42))
train_loader = torch.utils.data.DataLoader(train_set, batch_size=64)
val_loader = torch.utils.data.DataLoader(val_set, batch_size=256)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=256)
print("...done.")

print("initializing cINN...")
cinn = Reg_mnist_cINN()
cinn.to(device)
cinn.train()
print("...done.")

train_params = [p for p in cinn.parameters() if p.requires_grad]
optimizer = torch.optim.Adam(train_params, lr=1e-4, weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40], gamma=0.1)


print("preparing run directory...")
os.mkdir(run_dir)
os.mkdir(os.path.join(run_dir, "checkpoints/"))
print("...done")

print("epoch \t batch \t total_loss \t rec_loss \t prior_loss \t jac_loss")
for e in range(n_epochs):
    agg_nll = []
    # if e > 0: break
    for i, (v_field, cond) in enumerate(train_loader):
        # if i : break
        # print(i, "--------------------------------------------" )

        cond = cond.to(device)
        source = cond[:, :1, ...].to(device)
        target = cond[:, 1:, ...].to(device)

        z = torch.randn(batch_size, ndim_total).to(device)
        target_pred, v_field_pred, log_jac = cinn.reverse_sample(z, cond)

        rec_term = torch.mean(torch.sum(torch.square(target_pred - target), dim=(1, 2, 3))) / ndim_total

        z_prior, prior_jac = cinn(v_field_pred, c=cond)

        prior_nll = torch.mean(torch.sum(z_prior**2, dim=1) / 2)
        prior_jac = torch.mean(prior_jac)
        prior_term = (prior_nll - prior_jac)

        jac_term = torch.mean(log_jac) / ndim_total

        loss = rec_term - prior_term - jac_term

        rec_out = round(rec_term.item(), 2)
        prior_out = round(prior_term.item(), 2)
        p_1_out = round(prior_nll.item(), 2)
        p_2_out = round(prior_jac.item(), 2)
        jac_out = round(jac_term.item(), 2)
        loss_out = round(loss.item(), 2)

        print("{}\t{}\t{}\t{}\t{}\t{}".format(e, i, loss_out, rec_out, prior_out, jac_out))
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(train_params, 10.)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        if i % 100 == 0:
            checkpoint = {
                "state_dict": cinn.state_dict(),
                "optimizer_state": optimizer.state_dict(),
                "scheduler_state": scheduler.state_dict(),
            }
            torch.save(checkpoint, os.path.join(run_dir, 'checkpoints/model_{}.pt'.format(i)))

